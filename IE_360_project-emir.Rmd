---
title: "IE 360 Project Report Group 9"
author: "Ali Oğuz Bilgiç - Musab Emir Baş - Selman Berk Özkurt - Yusuf Hançer"
date: "04 07 2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
require(jsonlite)
require(httr)
require(data.table)
get_token <- function(username, password, url_site){
  
  post_body = list(username=username,password=password)
  post_url_string = paste0(url_site,'/token/')
  result = POST(post_url_string, body = post_body)
  
  # error handling (wrong credentials)
  if(result$status_code==400){
    print('Check your credentials')
    return(0)
  }
  else if (result$status_code==201){
    output = content(result)
    token = output$key
  }
  
  return(token)
}

get_data <- function(start_date='2020-03-20', token, url_site){
  
  post_body = list(start_date=start_date,username=username,password=password)
  post_url_string = paste0(url_site,'/dataset/')
  
  header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
  result = GET(post_url_string, header, body = post_body)
  output = content(result)
  data = data.table::rbindlist(output)
  data[,event_date:=as.Date(event_date)]
  data = data[order(product_content_id,event_date)]
  return(data)
}


send_submission <- function(predictions, token, url_site, submit_now=F){
  
  format_check=check_format(predictions)
  if(!format_check){
    return(FALSE)
  }
  
  post_string="list("
  for(i in 1:nrow(predictions)){
    post_string=sprintf("%s'%s'=%s",post_string,predictions$product_content_id[i],predictions$forecast[i])
    if(i<nrow(predictions)){
      post_string=sprintf("%s,",post_string)
    } else {
      post_string=sprintf("%s)",post_string)
    }
  }
  
  submission = eval(parse(text=post_string))
  json_body = jsonlite::toJSON(submission, auto_unbox = TRUE)
  submission=list(submission=json_body)
  
  print(submission)
  # {"31515569":2.4,"32939029":2.4,"4066298":2.4,"6676673":2.4,"7061886":2.4,"85004":2.4} 
  
  if(!submit_now){
    print("You did not submit.")
    return(FALSE)      
  }
  
  
  header = add_headers(c(Authorization=paste('Token',token,sep=' ')))
  post_url_string = paste0(url_site,'/submission/')
  result = POST(post_url_string, header, body=submission)
  
  if (result$status_code==201){
    print("Successfully submitted. Below you can see the details of your submission")
  } else {
    print("Could not submit. Please check the error message below, contact the assistant if needed.")
  }
  
  print(content(result))
  
}

check_format <- function(predictions){
  
  if(is.data.frame(predictions) | is.data.frame(predictions)){
    if(all(c('product_content_id','forecast') %in% names(predictions))){
      if(is.numeric(predictions$forecast)){
        print("Format OK")
        return(TRUE)
      } else {
        print("forecast information is not numeric")
        return(FALSE)                
      }
    } else {
      print("Wrong column names. Please provide 'product_content_id' and 'forecast' columns")
      return(FALSE)
    }
    
  } else {
    print("Wrong format. Please provide data.frame or data.table object")
    return(FALSE)
  }
  
}

# this part is main code
subm_url = 'http://167.172.183.67'

u_name = "Group9"
p_word = "cyAmVBA2I7GSM8Mm"
submit_now = FALSE

username = u_name
password = p_word

token = get_token(username=u_name, password=p_word, url=subm_url)
data = get_data(token=token,url=subm_url)

predictions=unique(data[,list(product_content_id)])
predictions[,forecast:=2.3]
send_submission(predictions, token, url=subm_url, submit_now=F)

```

## Introduction

This project is about forecasting the number of sales of the next day. Trendyol provided past data for that project. Historical recordings of 8 different products were given with their id, date of the event, price, number of products sold, number of visits to the page of the product, number of times that products were put into the basket of the customer, number of times that product was signed as favourite, sales numbers of the category, sales numbers of the category for that brand, visit numbers of the category and visit number of Trendyol. After that, it is required to estimate sales number of the next day for those products. Product definitions are shared in Table 1.

![Table 1. Product Definitions](https://img.imageupload.net/2020/07/06/Table1.png)

In the project definition, importance of other details such as day of the week or special occasions was specified. With this object, site level data was equipped. Black Friday or similar events are determinable after that. It was announced that in the process of estimating, extraneous goods are also useable. 
Prepared application programming interface lets this project to be performed as a competition. Everyday submissions are required after having, manipulating and developing  forecast models. Apparently, model is applicable in case of arranging sufficient inventory or service degrees. 


## Related Literature

Before preparing this project, it was required to study on DataCamp courses to understand the essence of the R for time series. There, besides all other courses, “Manipulating Time Series Data With xts and zoo in R”, “Time Series Analysis in R”, “Forecasting in R”, and “Forecasting Product Demand in R” courses were studied for that specific project.
Terms like xts, White Noise (WN), Random Walk (RW), Trends, seasonality, stepwise modelling and cyclicity, or Mean Absolute Error (MAE),  Mean Absolute Percentage Error (MAPE) were studied on to shape the project model better with Autoregression (AR), Simple Moving Average (MA) and ARIMA(AutoRegressive Models Integrated Moving Average). Those applications will be used and explained in the following parts.

The most significantly studied literature in this work is related to stepwise regression. This was made to allow building more generic models without the need to specify model building parameters for every product individually. After investigating various possibilities of the procedure, a rather orthodox technique was settled on, brief information about which can be found [here](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Stepwise_Regression.pdf).


## Approach

At the project, beginning code was provided. It ensured the code submissions to the application programming interface for each product separately with group number on it. Then the code for the estimation of the number of sales for the next day begins. In the process of coding, necessary libraries like xts, gtrendsR or data.table were added.


### Data Structuring
It was decided that the data is to be stored as xts objects instead of relational data tables because of relative simplicity for time series analysis and lack of need for operations involving relational algebra. Then, regardless of whether the same model is to be used for all products of question or not, it was found more effective to store data related to products in separate tables for ease of processing. This necessitated storing individual xts tables including all regressors and past data in an R list sata structure and doing any subsequent operations looping through the list. Abstraction of variation of products into lengths of lists also provided basis for generalization of the same code for arbitrary sets of products.


At the beginning of the study, number of products were checked. Those 8 products were needed to be split up to work on each item distinctively so we created distinct models. Thus, a list named ‘xdata’ was created to save each product separately day by day with the help of a for loop. In that loop, for every product, an xts object is created and added to the ‘xdata’. There is an xts object for each product that was provided within the past data.
```{r,  warning=FALSE , message=FALSE}
products<-unique(data$product_content_id)
require(xts)
xdata<-list()
for(i in 1:length(products)){
  xdata[[products[i]]]<-xts(data[product_content_id==products[i],c(1,4:11)],
                            order.by = data[product_content_id==products[i]]$event_date)
}
```


### Data Padding
In order to train and predict data on the same table, which would be convenient, the table was extended to include all dates including the day to be predicted, which is set to be tomorrow according to the system date. For the initial formation of this extended table, last-observation-carry-forward was employed for the missing data, which could be changed later. This enabled concurrent development of the rest of the project before a better extrapolation model could be built.

Provided data was not containing today's and tomorrow's data. To prevent this situation to create greater problems, these days' data are fulfilled. . After that, there is not any blanks in the data. Sales estimates might be worked on.
```{r,  warning=FALSE , message=FALSE}
for(i in 1:8){
  l<-last(index(xdata[[i]]))
  while(l<=Sys.Date()){
    l<-l+1
    xdata[[i]]<-rbind(xdata[[i]],xts(coredata(last(xdata[[i]])),l))
  }
}
```
 

### Trend 
One predictor to be composited with other predictors fusing various time series analysis techniques was the regular trend component. After visual inspection of the data for all given products, a linear regression horizon of 21 days was found reasonable to avoid weekday bias and including short run fluctuations in trend component, while absorbing annual seasonal components into the trend. 

For estimating sales sold_count is a strong element. As first and basic estimation tool, linear model of sold_count was preferred. Linear component of each item was added with the data of the last 21 days by examining sold_count data as a time series. Then, for every product, a trend component was added as an attribute to be used as a regressor thereafter. For that calculation, lasttrend function was preferred.

In order to avoid training bias, the trend components for all training and prediction data are generated as a linear extrapolation of past data up to to days before the date of the datum. 
```{r,  warning=FALSE , message=FALSE}
lasttrend<-function(series,h,myindex){
  rangelast<- (myindex-2-h):(myindex-2)
  localmodel<-lm(series[as.Date(rangelast)]$sold_count ~ as.integer(index(series[as.Date(rangelast)])),na.action="na.omit")
  as.numeric(localmodel$coefficients[1]+localmodel$coefficients[2]*as.integer(myindex))
}
HHHH<-21
for(i in 1:8){
  xdata[[i]]<-cbind(xdata[[i]],trend=NA)
  for (j in (HHHH+3):dim(xdata[[i]])[1]){
    xdata[[i]][j,"trend"]<-lasttrend(xdata[[i]],HHHH,index(xdata[[i]][j]))
  }
}
``` 
 
 
### Data Cleaning
Spline interpolation was used for missing data because it provides reasonable estimates and is a very robust technique. These missing data was mostly due to no sales being achieved in some days, leaving the effective sale price undetermined.  
```{r,  warning=FALSE , message=FALSE}
for(i in 1:8){
  xdata[[i]][which(xdata[[i]][,"price"]<0),"price"]<-NA
  xdata[[i]][,"price"]<-na.spline(xdata[[i]][,"price"])
}
``` 
 
 
### GTrends 
Visually, strong correlation of Google Trends data for tokens related to the products were observed, partly not described in the given data. This was enhanced substantially by the fact that the annual seasonality can not be inferred from the given data set due to its short duration. Google Trends, however, showed very strong annual seasonality for a very long time which could be used in the model. It should be noted, however, that this correlation varies significantly with respect to the individual products. This strongly supported the previous idea that each product type should be modelled separately. 

The google trends data is supplied daily for tha last months and weekly for the past years. In order to preserve all the information in the data the short and long run data from Google Trends are independently downloaded and then merged, independent of whether it is going to be useful in the current version of the model building software. This was made to save the possibility to build more advanced models in the future.

```{r , warning=FALSE , message=FALSE}
require(gtrendsR)
#import google trends data
gtrends <- xts()
gtrends_m <- xts()
tformat(gtrends) = "%Y-%m-%d"
tformat(gtrends_m) = "%Y-%m-%d"
search <- c("Tayt", "Şarj Edilebilir Diş Fırçası", "Mont", "Islak Mendil", 
            "Bikini", "Kablosuz  Kulaklık", "Elektrikli Süpürge", "Yüz Temizleyici")
for(i in 1:8){
  r<-gtrends(search[i], geo="TR",time="today+5-y")$interest_over_time
  r[,1] <- as.Date(r[,1], format = "%Y-%m-%d")
  date <- r[,1]
  date <- as.Date(setdiff(date,last(date,"3 months")),format = "%Y-%m-%d")
  date <- as.Date(setdiff(date,last(date,"1 week")),format = "%Y-%m-%d")
  gtrends<-cbind(gtrends,xts(r[,2], order.by = r[,1])[date])
  x<-gtrends(search[i], geo="TR",time="today 3-m")$interest_over_time
  x[,1] <- as.Date(x[,1], format = "%Y-%m-%d")
  gtrends_m<-cbind(gtrends_m,xts(x[,2], order.by = x[,1]))
}
colnames(gtrends)<-c("1","2","3","4","5","6","7","8")
colnames(gtrends_m)<-c("1","2","3","4","5","6","7","8")
gtrends <- rbind(gtrends, gtrends_m)
```
 
```{r, warning=FALSE , message=FALSE}
for(i in 1:8){
  xdata[[i]]<-cbind(xdata[[i]],gtrends[,i])
  colnames(xdata[[i]])[length(colnames(xdata[[i]]))]<-"gtrends"
  xdata[[i]][,"gtrends"]<-na.locf(xdata[[i]][,"gtrends"])
}
``` 


### Predictor Filling  
Estimating today’s sales numbers from the data of the two days before was another idea. xdata_lag list was created for that purpose. To be used before predicting regressors, from the data of two days before, sold_count data of the today was tried to predict.
```{r, warning=FALSE , message=FALSE}
xdata_lag <- list()
for(i in 1:8){
  xdata_lag[[i]] <- lag(xdata[[i]][,-2], k = 2)
  xdata_lag[[i]] <- cbind(xdata_lag[[i]], xdata[[i]][,2])
}
``` 


### Alternative Regressors 
Another approach was the importance of the last 3 weeks’ weekly data for each day. So this approach was for the last 22 days. sold_count data of 7 days before, 14 days before and 21 days before were added for each product everyday. This step was applied both to xdata and xdata_lag. If effectiveness of that approach is not satisfying, in the loop which was written to create a linear model, it was not going to be used.Output of the part until this point and output of the continuing part were compared and according to observations more dependable one submitted.
```{r,  warning=FALSE , message=FALSE}
  oneweek<-xts(NA,Sys.Date())
twoweeks<-xts(NA,Sys.Date())
threeweeks<-xts(NA,Sys.Date())

for(i in 1:8){
  xdata_lag[[i]]<-cbind(xdata_lag[[i]],oneweek,twoweeks,threeweeks)
  range<-which(!is.na(xdata_lag[[i]][,"sold_count"]))
  for(j in range){
    if(j<22)next
    xdata_lag[[i]][j,"oneweek"]<-xdata_lag[[i]][j-7,"sold_count"]
    xdata_lag[[i]][j,"twoweeks"]<-xdata_lag[[i]][j-14,"sold_count"]
    xdata_lag[[i]][j,"threeweeks"]<-xdata_lag[[i]][j-21,"sold_count"]
  }
  xdata[[i]]<-cbind(xdata[[i]],oneweek,twoweeks,threeweeks)
  range<-which(!is.na(xdata[[i]][,"sold_count"]))
  for(j in range){
    if(j<22)next
    xdata[[i]][j,"oneweek"]<-xdata[[i]][j-7,"sold_count"]
    xdata[[i]][j,"twoweeks"]<-xdata[[i]][j-14,"sold_count"]
    xdata[[i]][j,"threeweeks"]<-xdata[[i]][j-21,"sold_count"]
  }
}  
``` 


### Modelling 
Approaches that are improved till here were for creating a linear model. After using all, intercept was removed. Here, a while loop checks p values of results of approaches. Until every regressor has a p value less than 0.05 while loop works. One by one, linearly they are removed by beginning with the highest p value. The purpose of using stepwise regressor elimination instead of more complex regressor elimination methods is the fact that it would not be cost effective to consturct 16 different models. Finally, last model developed is returned. Models were formed for both xdata and xdata_lag. Models are built by lapply function.
```{r, warning=FALSE , message=FALSE}
predict<-rep(0,8)
predict_old <- rep(0,8)

mymodel<-function(xtable,range){
  df<-data.frame(coredata(xtable))   #first try a model of everything:
  try<-lm(sold_count ~ .-1,data=df[range,]-1)
  vars <- vector()
  while(mean(summary(try)$coefficients[,4] < 0.05) != 1){
    max <- max(summary(try)$coefficients[,4])
    vars<-append(vars,names(which(summary(try)$coefficients[,4] == max)))
    new<-as.formula(paste("sold_count ~.-", paste(vars,collapse="-"),"-1")) 
    try <- lm(new,data=df[range,])
  }
  try
} 

models<-lapply(xdata_lag,mymodel,1:(dim(xdata_lag[[8]])[1]-1))
oldmodel <- lapply(xdata,mymodel,1:(dim(xdata_lag[[8]])[1]-3))

lapply(models,summary)
lapply(oldmodel,summary)
``` 
 
 
### Predicting
Predictions were made and results were submitted. 
```{r, warning=FALSE , message=FALSE}
real<-rep(0,8)
#predict
for(i in 1:8){
  predict[i]<-predict(models[[i]],newdata=last(data.frame(coredata(xdata_lag[[i]]))))
  predict_old[i] <- predict(oldmodel[[i]],newdata=last(data.frame(coredata(xdata[[i]]))))
  real[i]<-last(xdata_lag[[i]])$sold_count
}
predict
for(i in 1:8){
  if(predict[i] <0){
    predict[i] <- 0
  }
} 
``` 


### Model Selecting 
From observations, products provided better results for different approaches. In this case, for product 1,2,3 and 5, data of 2 days before gave more dependable results. However, for the rest, predicting with lags were more dependable. Submission were made accordingly. 
```{r, warning=FALSE , message=FALSE}
predict[1] <- predict_old[1]
predict[2] <- predict_old[2]
predict[3] <- predict_old[3]
predict[5] <- predict_old[5]
``` 


## Results & Conclusion
It is determined that while automatization of model building reduces human labor put into the task, it yielded inferior results compared to hand-tuned model-building parameters. Taking into account the facts that it would be impossible to manually build models for a large set of such independent systems, and also that even most manual models need computer optimization to be successful; an efficient hybridization of the two approaches should be maintained. This study here was rather on the over-automated side of the spectrums. Yet, it is beneficial in the long term to try to automatize the as much as possible, due to advances in computing capabilities.

After all, it is clear that predicting future sales is a difficult task for daily life. In times of crisis it is even harder. For that work, in that term, some daily results were really close to real number of sales though some days it was further. Covid 19 crisis, as expected, changed norms of the shopping and some days, number of sales waved in that one month as different from estimates according to last data. So, outliers are the real challenger here and predicting those days is a necessity by means of holding inventory and being prepared days before.
